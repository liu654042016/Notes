# cuda 编程基础概念

## 1.内存层次

```cpp
#include <stdio.h>

__global__ void square(float* d_out,float* d_in){
  int idx = threadIdx.x;
  float f = d_in[idx];
  d_out[idx] = f * f;
}

__global__ void increment_naive( int *g){
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    i = i % ARRAY_SIZE;
    g[i] = g[i] + 1;
}

__global__ void use_share_memory_GPU(float *array){
    int i, index = threadIdx.x;
    float average, sum = 0.0f;
    

    __share__ float sh_arr[128];

    sh_arr[index] = arry[index];

    __syncthreads();

    for(int i=0; i<index; i++){sum+=sh_arr[i];}
    average = sum / (index+1.0f);

    if(array[index]>average){array[ index] = average;}

    sh_arr[index] = 3.14
}

int main(int argc,char** argv){
  const int ARRAY_SIZE = 8;
  const int ARRAY_BYTES = ARRAY_SIZE * sizeof(float);

  // generate the input array on the host
  float h_in[ARRAY_SIZE];
  for(int i=0;i<ARRAY_SIZE;i++){
    h_in[i] = float(i);
  }
  float h_out[ARRAY_SIZE];

  // declare GPU memory pointers
  float* d_in;
  float* d_out;

  // allocate GPU memory
  cudaMalloc((void**) &d_in,ARRAY_BYTES);
  cudaMalloc((void**) &d_out,ARRAY_BYTES);

  // transfer the array to GPU
  cudaMemcpy(d_in,h_in,ARRAY_BYTES,cudaMemcpyHostToDevice);

  // launch the kernel
  square<<<1,ARRAY_SIZE>>>(d_out,d_in);

   //use_shared_memory_GPU<<1,128>>(d_arr);

  // copy back the result array to the GPU
  cudaMemcpy(h_out,d_out,ARRAY_BYTES,cudaMemcpyDeviceToHost);

  // print out the resulting array
  for(int i=0;i<ARRAY_SIZE;i++){
    printf("%f",h_out[i]);
    printf(((i%4) != 3) ? "\t" : "\n");
  }

  // free GPU memory allocation
  cudaFree(d_in);
  cudaFree(d_out);

  return 0;

}

```

* 本地内存
```cpp
__global__ void use_local_memory_GPU(float in){
    float f;
    f = in;
}
use_local_memory_GPU<<1,128>>(2.0f);

```
* 共享内存
```cpp
__global__ void use_share_memory_GPU(float *array){
    int i, index = threadIdx.x;
    float average, sum = 0.0f;
    

    __share__ float sh_arr[128];

    sh_arr[index] = arry[index];

    __syncthreads();

    for(int i=0; i<index; i++){sum+=sh_arr[i];}
    average = sum / (index+1.0f);

    if(array[index]>average){array[ index] = average;}

    sh_arr[index] = 3.14
}
 use_shared_memory_GPU<<1,128>>(d_arr);
```


* 全局内存
```cpp
__global__ void use_global_memory_GPU(float *array){
    array[threadIdx.x] = 2.0f*(float)threadIdx.x;
}
//invoke kernel func
float h_arr[128];
float *d_arr;

cudaMalloc((void**)&d_arr, sizeof(float)*128);
cudaMemcpy((void*)d_arr,(void*)h_arr, sizeof(float)*128, cudaMemcpyHostToDevice);
use_global_memory_GPU<<1,128>>(d_arr);
cudaMemcpy((void*)h_arr, (void*)d_arr, sizeof(float)*128, cudaMemcpyDeviceToHost);
```

## 2.CUDA 同步操作
* 原子操作
让 10000 个线程增加 10 个数组元素
```cpp
__global__ void increment_naive( int *g){
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    i = i % ARRAY_SIZE;
    g[i] = g[i] + 1;
}
//这个应该是错的，因为各个线程相加的话结果不对
__global__ void increment_atomic( int *g){
    int i = blockIdx.x*blockDim.x + threadIdx.x;
    i = i % ARRAY_SIZE;
    atomicAdd(& g[i], 1);
}

```
* 同步函数

_syncthreads()//对 grid 所有线程可见
_threadfence()
_threadfence_block() //对 block 所有线程可见


```cpp
__global__ void use_share_memory_GPU(float *array){
    int i, index = threadIdx.x;
    float average, sum = 0.0f;
    

    __share__ float sh_arr[128];

    sh_arr[index] = arry[index];

    __syncthreads();

    for(int i=0; i<index; i++){sum+=sh_arr[i];}
    average = sum / (index+1.0f);

    if(array[index]>average){array[ index] = average;}

    sh_arr[index] = 3.14
}
 use_shared_memory_GPU<<1,128>>(d_arr);

```


* cpu/gpu同步
cudaStreamSynchronize()
cudaEventSynchronize()
## 并行化高效策略
1. 归约策略
求数组和
```cpp
//使用 global memory
__global__ void global_reduce_kernel(float *d_out, float *d_in){
    int myId =  threadIdx.x + blockDim.x * blockIdx.x;
    int tid = threadIdx.x;
    // do reduction in global mem
    for(unsigned int s = blockDim.x / 2; s > 0; s>>=1){
        if(tid<s){
            d_in[myId]+=d_in[myId+s];
        }
        __syncthreads()
    }
    if(tid==0){
        d_out[blockIdx.x] = d_in[my_Id];
    }
}
//使用 share memory
__global__ void shmem_reduce_kernel(float *d_out, float *d_in){
    extern __shared__ float = sdata[];
    int myId = threadIdx.x + blockDim.x*blockIdx.x;
    int tid = threadIdx.x;
    //local shared mem from global
    sdata[tid] = d_in[myId];
    __syncthreads();

    for (unsinged int s = blockDim.x/2; s>0; s>>=1){
        if(tid < s){
            sdata[tid]+=sdata[tid+s];
        }
        __syncthreads();
    }
    if(tid==0){
        d_out[blockIdx.x] = sdata[0];
    }
}
```
2. 扫描策略
```cpp
// 全局内存
__global__ void global_scan(float* d_out, float* d_in){
    int idx = threadIdx.x;
    float out = 0.00f;
    d_out[idx] = d_in[idx];
    __syncthreads();
    for(int interpre = 1; interpre<sizeof(d_in);interpre*=2){
        if(idx-interpre>=0){
            out = d_out[idx] + d_out[idx-interpre];
        }
        __syncthreads();
        if(idx-interpre>=0){
            d_out[idx] = out;
            out = 0.00f;
        } 
    }
}

共享内存
__global__ void shmem_scan(float* d_out, float* d_in){
    extern __shared__ float sdata[];
    int idx = threadIdx.x;

    //local shared from gloabl
    sdata[idx] = d_in[idx];
    __syncthreads();
    for( int interpre=1; interpre<sizeof(d_in); interpre*=2){
        if(idx-interpre>=0){
            out = sdata[idx] + sdata[idx-interpre];
        }
        __syncthreads();
        if(idxinterpre>=0){
            sdata[idx] = out;
            out = 0.0f;
        }
    }
    if(idx == threadDim.x){
        d_out[idx] = sdata[idx];
    }
}
```


































































*
